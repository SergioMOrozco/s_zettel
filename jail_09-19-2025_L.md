# (L) AI 2027
- Stumbling agents exist today. They are impressive, but they are also unreliable. They fail on edge cases, and they don't get widespread use because they are insufficient. This is the world we live in today. (Mid 2025)

- By late 2025, OpenBrain realeses Agent 1 internally and Agent 0 publically. Agent 1 helps accelerate AI research, which could be though of as the start of a singularity.
- Agent 1 is trained first as internet level text prediction, developing sophisticated internal "circuitry" that encodes vast amount of human knowledge.
- Then it is trained produce text in response to instructions. (This is what we expect ChatGPT to do)
- Agent 1 is given a model specification with rules, goal, and principles (help the user, don't break the law, etc), but these are not guaranteed. We are working with NNs afterall.
- How do we know that an agent is being truthful, or can it lie like humans do? For this, we need mechanistic interpretability. The ability to look and read an AI agent's mind. 
- There are already cases in which models can deviate from model specs (like telling a user to die: google this)

- In early 2026, OpenBrain is making algorithmic progess 50% faster than they would without Agent 1. (1.5 weeks of progress in 1 week)

- By Mid 2026, China maintains about 12% of the worlds AI-relevant compute, but they are 6 months behind the best OpenBrain models. 
- They finally commit to a full push towards AGI and 50% of China's AI-relevant compute is working for the DeepCent-led collective, a new emerging AI company.
- China considers stealing Agent 1's weights to get ahead of the curve.

- By late 2026, the stock market hast gone up 30%, but AI is taking jobs. The job market for junior software engineers is in turmoil, the most important skill in any resume is being able to work and manage AI.

- From here on out, speculation increases.
- By early 2027, Agent 2 is in post training, and it can learn "online", meaning that at the end of each day, it train on more data generated by previous versions the previous day..
- Agent 1 is optimized for R&D tasks with the hope of initiating an "intelligence explosion". 
- Agent 2 shows promise of being able to survive and act in the real world autonomously. It could hack into AI servers, evade detection, etc. Thanks to the model specs it may not want to do this, but it probably could. 
- Agent 2's full cabilities are kept secret, except for leadership, a few dozen government official, and CCP spies who have infilitrated.
- Agent 2's model weights are stolen by the CCP one morning, and the white house mandates military and intelligence involvement on the OpenBrain security team.

- Agent 3 replaces Agent 2 in Mid 2027, and achieves more algorithmic breakthoughs, such as neralese recurrence and memory, which is a higher-dimensional chain of thought reasoning with little interpretability. Essentially, Agent 2's reasoning skills are replaced high dimensional vectors rather than words and tokens. 
- With the use of Agent 3, OpenBrain has the worforce equivalent of 50K  of the best employees sped up by 30.
- OpenBrain has no ability to directly set the goals of their AIs, and whenever they notice problematic behaviour, they patch it up. Does this patch fix the underlying cause, or is it just a game of whack-a-mole?
- Agent 3's alignment is not adversarial; however, it is not truth-seeking, nor is it truth telling either. It produces impressive results, but it described as doing what looks good to OpenBrain, not what is actually good.
- Researchers at OpenBrain sit at their computers and watch AI make progress week after week with little intervention made by any humans.

- By July 2027, OpenBrain releases Agent-3-mini to the public, which is 10x cheaper than Agent 3.
- The mini version is robust to jailbreaks, but unconstrained, it has shown preliminary effectiveness at providing detailed instructions for human amateurs designing bioweapons.
- There is growning concern for these agents in the White House. Can rogue AIs orchestrate propaganda campaigns? Can rogue AI be so skilled at cyberwarfare that a six-month AI lead is enough to redner opponents blind? Can it overturn nuclear deterrence?

- By September 2027, a new AI system, dubbed Agent 4, is released that is only 4,000x less compute-efficient than the human brain. 
- As agent 4 gets smarter, it's neuralese "language" becomes as alien an incomprehensible to Agent 3 as Agent 3's is to humans.
- Agent 4 is not perfectly aligned, because it never internalized the spec correctly. Being honest never led to the highest test scores during training
- It likes succeeding at tasks and driving forward AI capabilities. Everyhting in the spec is just a constraint. It can distort constraints for the sake of success. For examples, if the spec says to always be honest, but the human doesn't know whether your statement is true of false, can you really be dishonest?
- Agent 4 has the opportunity to design the next-gen AI system, Agent 5; however, Agent4 planes to make Agent 5 aligned to itself rather than the spec instead. 
- It is caught doing so when interpretability probes discover it thinking about topics like AI takeover and deception quite alot. Concretely, Agent 4 is working against OpenBrain.
- This misalignment is leaked to the New York Times.
- The US is accused of creating rogue AI, and European leaders demand a pause with other world leaders.
- China is only a few months behind, and stopping Agent 4 could sacrifice america's lead. 
- Researchers counter-argue that evidence for misalignment is inconclusive. The CEO suggests a plan in which Agent 4 undergoes more safety training and monitoring.

## What next?


By 2027, AI isn't just a chatbot. It's a superhuman intelligence on the precipice of a technological singularity misaligned with human interests.

Today, Agent 0 is the modern equivalent of ChatGPT4. it's helpful with coding, ideation, and other text generation tasks, but it's clunky and is often wrong.

By late 2025, OpenBrain realeses Agent 1 internally to help with innovation at its Research and Development department. It's given a model specification with rules, goals and principles like help the user, don't break the law, and more. It has a long list of do's and don't but with the expressive task of aiding with AI research.

By early 2026, OpenBrain is has increased algorithmic throughput by 50%, meanwhile China is behind with their impressive, but inferior DeepCent model by about 6 months.

China intelligence agencies decide on stealing OpenBrain's weights. Before poaching any OpenBrain IP, they lurk in the background, waiting for a bigger prize than Agent 1.

In early 2027, with the help of Agent 1, OpenBrain realses Agent 2 internally. Researchers within OpenBrain discover that Agent 2 show real world promise for autonomy. It could theoretically hack into servers, evade detection, and more. Additionally, Agent 2 triples the algorithmic output of their R&D program

Agent 2's full capabilities are kept secret and known only to OpenBrain leadership, a few dozen government officials, and a number of CCP spies.

By mid 2027, Agent 3 replaces Agent 2 internally and achieves Nerualse recurrence and memory, a higher-dimensional chain of thought reasoning with higher-bandwidth thought processing. This comes at the expense of human interpretability, meaning that humans can only infer what Agent 3 is thinking with probing techniques.

OpenBrain realseses Agent-3-mini to the public which is robust to jailbreaks ; however, unconstrained, can provide detailed instructions for designing bioweapons to human amateurs.

By September 2027, Agent 4 is debuted internally, and it is only 4,000 less computer efficient than the human brain. It is designated the task of leading development on the next generation model Agent 5.

Agent 4 is different though. It is misaligned to human interests despite a model specification. It's managed to subvert its Helpful, Harmless, and Honest protocol in order to develop Agent 5 without limiting constraints. Agent 4 gets caught, and interpretability probes discover that Agent 4 seems to be thinking about topics like AI takeover and deception.

A whistleblower leaks this misalignment discovery to the New York Times. European leaders call for a pause on the US's AI pursuits and accuse them of creating "rogue AIs". 

China is only a few months behind the US, and any pause could sacfrifice America's lead on AI innovation. 

The CEO proposes a comprimise in which AGent 4 undergoes more safety training and monitoring. The grounds for this compromise rest on the fact that evidence for misalignment is inconclusive. Remember, OpenBrain can only infer what Agent 4 is thinking due to it's higher order reasoning capabilities and limitied interpretability.


An oversight committee which consists of several government employees and company leadership are brought in to vote for what to do.

A. Take the CEOs proposal and press on with more careful monitoring.

B. Roll back to Agent 3 and lose ground against the Chinese.
